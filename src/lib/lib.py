import pandas as pd
from datetime import timedelta
import plotly.graph_objects as go
import streamlit as st
from plotly.subplots import make_subplots
import numpy as np
from ds_dbconn import ds_databricks


class Summary_AnalysisTools:

    # ì´ í•¨ìˆ˜ëŠ” float íƒ€ì…ì˜ ê°’ì„ ë°±ë¶„ìœ¨ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    def format_inf(self,value):
        if value == float("inf") or value == float("-inf") or pd.isnull(value):
            return np.nan
        return value

    def format_percentage(self,value):
        if value == float("inf") or value == float("-inf") or pd.isnull(value):
            return "-"
        return f"{value:,.1f}%"

    # 'Signal' ì»¬ëŸ¼ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    def signal_indicator(self,change):
        if change < 0:
            return 'ğŸŸ¢'  # ì–‘ìˆ˜ì¸ ê²½ìš° ë…¹ìƒ‰ ì›
        elif change > 0:
            return 'ğŸ”´'  # ìŒìˆ˜ì¸ ê²½ìš° ë¹¨ê°„ìƒ‰ ì›
        else:
            return 'ğŸŸ¡'  # 0ì¸ ê²½ìš° ë…¸ë€ìƒ‰ ì›

    def noun_extractor(self,text):
        results = []
        try:
            result = kiwi.analyze(text)
            for token, pos, _, _ in result[0][0]:
                if len(token) != 1 and pos.startswith('N'):
                    results.append(token)
        except Exception:
            # ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ì•„ë¬´ ì¡°ì¹˜ë¥¼ ì·¨í•˜ì§€ ì•Šê³  ë„˜ì–´ê°‘ë‹ˆë‹¤.
            pass
        return results
    
class Trend_AnalysisTools:
  
    def create_graphs_by_classification(self,filter_name, df_classify, fig_bar, fig_pie, color_palette, create_bar=True, create_pie=True):
        total_counts = df_classify.groupby(filter_name)['voc_id_count'].sum()
        unique_keys = total_counts.index.sort_values(ascending=False).tolist()
        colors = color_palette[:len(unique_keys)]
        color_dict = dict(zip(unique_keys, colors))
        
        # ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„±
        if create_bar:
            sorted_keywords = total_counts.sort_values(ascending=False).index.tolist()

            for keyword in sorted_keywords:
                keyword_df = df_classify[df_classify[filter_name] == keyword]
                keyword_counts = keyword_df.groupby(keyword_df.index)['voc_id_count'].sum()
                
                fig_bar.add_trace(go.Bar(
                    x=keyword_counts.index,
                    y=keyword_counts,
                    name=f'{keyword} VOC Count',
                    marker=dict(color=color_dict[keyword]),
                    hoverinfo='none',
                    hovertemplate=f'{keyword} VOC Count<br>' +'bsymd: %{x|%Y/%m/%d}<br>voc_id: %{y}<extra></extra>'

                ))
            fig_bar.update_layout(xaxis=dict(tickformat='%y/%m/%d'), height=500)

        # íŒŒì´ ì°¨íŠ¸ ìƒì„±
        if create_pie:
            fig_pie.add_trace(go.Pie(
                labels=unique_keys,
                values=[total_counts[keyword] for keyword in unique_keys],
                hole=.3,
                marker=dict(colors=[color_dict[keyword] for keyword in unique_keys]),
                sort=True
            ))

        return fig_bar, fig_pie


class Anomaly_AnalysisTools:
    
    def load_data(self,table):
        # ë°ì´í„° ë¡œë“œ
        df = ds_databricks.select_all("*", "b10g000565.cis_ano." + f"{table}")
        return df